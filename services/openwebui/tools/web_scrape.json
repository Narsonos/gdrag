{"id":"web_scrape","user_id":"4b6f4ccb-a842-4ea7-8f42-8ad7f0c90092","name":"Web scrape","content":"\"\"\"\ntitle: Simple Web Scrape\nauthor: Narsonos\ndescription: A simple web scraping tool, that does not use jina as the original one.\noriginal_author: Pyotr Growpotkin\noriginal_author_url: https://github.com/christ-offer/\noriginal_git_url: https://github.com/christ-offer/open-webui-tools\nversion: 0.0.1\nlicense: None\nrequirements: httpx, bs4\n\"\"\"\n\nimport httpx\nfrom typing import Callable, Any\nimport re, bs4\nfrom pydantic import BaseModel, Field\n\nimport unittest\n\n\ndef extract_title(text):\n    \"\"\"\n    Extracts the title from a string containing structured text.\n\n    :param text: The input string containing the title.\n    :return: The extracted title string, or None if the title is not found.\n    \"\"\"\n    match = re.search(r\"<title>(.*?)</title>\", text, re.IGNORECASE | re.DOTALL)\n    return match.group(1).strip() if match else None\n\n\ndef clean_urls(text) -> str:\n    \"\"\"\n    Cleans URLs from a string containing structured text.\n\n    :param text: The input string containing the URLs.\n    :return: The cleaned string with URLs removed.\n    \"\"\"\n    return re.sub(r\"\\((http[^)]+)\\)\", \"\", text)\n\n\nclass EventEmitter:\n    def __init__(self, event_emitter: Callable[[dict], Any] = None):\n        self.event_emitter = event_emitter\n\n    async def progress_update(self, description):\n        await self.emit(description)\n\n    async def error_update(self, description):\n        await self.emit(description, \"error\", True)\n\n    async def success_update(self, description):\n        await self.emit(description, \"success\", True)\n\n    async def emit(self, description=\"Unknown State\", status=\"in_progress\", done=False):\n        if self.event_emitter:\n            await self.event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": status,\n                        \"description\": description,\n                        \"done\": done,\n                    },\n                }\n            )\n\n\nclass Tools:\n    class Valves(BaseModel):\n        pass\n\n    class UserValves(BaseModel):\n        CLEAN_CONTENT: bool = Field(\n            default=True,\n            description=\"Remove links and image urls from scraped content\",\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n\n    async def web_scrape(\n        self,\n        url: str,\n        __event_emitter__: Callable[[dict], Any] = None,\n        __user__: dict = {},\n    ) -> str:\n        \"\"\"\n        Scrape a web page and extract text using BeautifulSoup.\n\n        :param url: The URL of the web page to scrape.\n        :return: The scraped and processed webpage content, or an error message.\n        \"\"\"\n        emitter = EventEmitter(__event_emitter__)\n        if \"valves\" not in __user__:\n            __user__[\"valves\"] = self.UserValves()\n\n        await emitter.progress_update(f\"Scraping {url}\")\n\n        try:\n            async with httpx.AsyncClient(timeout=10) as client:\n                response = await client.get(url)\n                response.raise_for_status()\n        except httpx.HTTPError as e:\n            exception_desc = str(e)\n            error_message = f\"Error occured while scraping: {exception_desc if exception_desc else e.__class__.__name__}\"\n            await emitter.error_update(f\"Failed to scrape: {error_message}\")\n            return error_message\n\n        await emitter.progress_update(\"Received content, cleaning up ...\")\n        # NOTE: I decided to stick with BS4 instead of trafilatura, since it does not catch what i'd like to catch for my targets\n        soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n        title_tag = soup.find(\"title\")\n        title = title_tag.get_text(strip=True) if title_tag else None\n\n        # Removing modals and trash headers\n        for tag in soup([\"script\", \"style\", \"header\", \"nav\", \"head\", \"footer\"]):\n            tag.decompose()\n        for popup in soup.find_all(attrs={\"aria-modal\": \"true\"}):\n            popup.decompose()\n        for popup in soup.find_all(attrs={\"role\": \"dialog\"}):\n            popup.decompose()\n\n        # Searching for footer comment and removing all beyond\n        html_str = str(soup)\n        footer_index = html_str.find(\"<!--footer-->\")\n        if footer_index != -1:\n            truncated_html = html_str[:footer_index]\n            soup = bs4.BeautifulSoup(truncated_html, \"html.parser\")\n        text = soup.get_text(\" \", strip=True)\n\n        content = clean_urls(text) if __user__[\"valves\"].CLEAN_CONTENT else text\n        await emitter.success_update(f\"Successfully Scraped {title if title else url}\")\n        return (\"Title: \" + title + \"\\n\" if title else \"\") + content\n\n\nclass WebScrapeTest(unittest.IsolatedAsyncioTestCase):\n    async def test_web_scrape(self):\n        url = \"https://toscrape.com\"\n        content = await Tools().web_scrape(url)\n        self.assertTrue(content.startswith(\"Title: Scraping Sandbox\"))\n\n\nif __name__ == \"__main__\":\n    print(\"Running tests...\")\n    unittest.main()\n","specs":[{"name":"web_scrape","description":"Scrape a web page and extract text using BeautifulSoup.","parameters":{"properties":{"url":{"description":"The URL of the web page to scrape.","type":"string"}},"required":["url"],"type":"object"}}],"meta":{"description":"Simple web scraper","manifest":{"title":"Simple Web Scrape","author":"Narsonos","description":"A simple web scraping tool, that does not use jina as the original one.","original_author":"Pyotr Growpotkin","original_author_url":"https://github.com/christ-offer/","original_git_url":"https://github.com/christ-offer/open-webui-tools","version":"0.0.1","license":"None","requirements":"httpx, bs4"}},"access_control":{},"updated_at":1757160103,"created_at":1757156442}